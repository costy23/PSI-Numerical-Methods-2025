{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chain Monte Carlo Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Using Markov Chain Monte Carlo to infer the parameters of a simple model\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "]add ForwardDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "]add LogExpFunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CairoMakie\n",
    "using Optim\n",
    "using ForwardDiff\n",
    "using LogExpFunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data set from  arxiv:1008.4686, table 1 (https://arxiv.org/abs/1008.4686)\n",
    "# You can also refer to that paper for more background, equations, etc.\n",
    "alldata = [201. 592 61; 244 401 25; 47  583 38; 287 402 15; 203 495 21; 58  173 15; 210 479 27;\n",
    "           202 504 14; 198 510 30; 158 416 16; 165 393 14; 201 442 25; 157 317 52; 131 311 16;\n",
    "           166 400 34; 160 337 31; 186 423 42; 125 334 26; 218 533 16; 146 344 22 ]\n",
    "# The first 5 data points are outliers; for the first part we'll just use the \"good\" data points\n",
    "x    = alldata[6:end, 1]\n",
    "y    = alldata[6:end, 2]\n",
    "# this is the standard deviation (uncertainty) on the y measurements, also known as \\sigma_i\n",
    "yerr = alldata[6:end, 3];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To start, let's have a look at our data set.\n",
    "f = Figure()\n",
    "Axis(f[1, 1], xlabel=\"x\", ylabel=\"y\")\n",
    "errorbars!(x, y, yerr);\n",
    "scatter!(x, y, markersize=20);\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we are going to imagine that there is some physical reason to believe there is a *linear* relationship between the quantities $x$ and $y$, so our *generative model* of the process is that there is a \"real\" or \"predicted\" value $y_{\\textrm{pred}}$ given by $y_{\\textrm{pred}} = b + m x$, for some parameters $b$ and $m$ that we will try to infer.  Our *measurements* $y$ are noisy measurements of $y_{\\textrm{pred}}$, with additive Gaussian noise with known variance, $\\sigma_i$ for data point $i$.  ($\\sigma$ is called `yerr` in this notebook.)  This is a strong assumption about our data-collection method.\n",
    "\n",
    "With those assumptions, we can write down the *likelihood* for a single measurement $y_i$ given its corresponding $x_i$ and $\\sigma_i$, and straight-line model parameters $b$ and $m$:\n",
    "\n",
    "$y_{\\textrm{pred},i} = b + m x_i$\n",
    "\n",
    "  $p(y_i | m, b) = \\frac{1}{\\sqrt{2 \\pi} \\sigma_i} \\exp(-\\frac{(y - y_{\\textrm{pred},i})^2}{2 \\sigma_i^2})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are making a number of simplifying assumptions here:\n",
    "\n",
    "* there are no uncertainties on the $x$ values\n",
    "* there are additive Gaussian measurement uncertainties on the $y$ values with known standard deviations $\\sigma$\n",
    "* the data points are statistically independent\n",
    "\n",
    "Since we are assuming the $x$ and $\\sigma$ values are perfectly known, we will treat them as *constants* rather than *data* in the probability equations.\n",
    "\n",
    "In practice, it is usually preferable to work in log-probabilities rather than\n",
    "linear probabilities, because the probability values can be very small, and if we're not careful we can hit a numerical issue called *underflow*, where the numbers become so small that they can't be represented in standard floating-point numerical representation.\n",
    "\n",
    "If the data points are statistically independent, then the likelihood of the whole collection of data points $y = \\{ y_i \\}$ is the *product* of their individual likelihoods:\n",
    "\n",
    "$p(y | m,b) = \\prod_i \\frac{1}{\\sqrt{2 \\pi} \\sigma_i} \\exp(-\\frac{(y - y_{\\textrm{pred},i})^2}{2 \\sigma_i^2})$\n",
    "\n",
    "and taking the log,\n",
    "\n",
    "$\\log p(y | m,b) = \\sum_i \\log(\\frac{1}{\\sqrt{2 \\pi} \\sigma_i}) -\\frac{(y - y_{\\textrm{pred},i})^2}{2 \\sigma_i^2}$\n",
    "\n",
    "The first thing we will do is implement that log-likelihood function!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function log_likelihood_one(params, x, y, yerr)\n",
    "    \"\"\"This function computes the log-likelihood of a data set with coordinates\n",
    "    (x_i,y_i) and Gaussian uncertainties on y_i of yerr_i (aka sigma_i)\n",
    "\n",
    "    The model is a straight line, so the model's predicted y values are\n",
    "        y_pred_i = b + m x_i.\n",
    "\n",
    "    params = (b,m) are the parameters (scalars)\n",
    "    x,y,yerr are arrays (aka vectors)\n",
    "\n",
    "    Return value is a scalar log-likelihood.\n",
    "    \"\"\"\n",
    "    # unpack the parameters\n",
    "    b,m = params\n",
    "    # compute the vector y_pred, the model predictions for the y measurements\n",
    "    y_pred = b .+ m .* x\n",
    "    # compute the log-likelihoods for the individual data points\n",
    "    # (the quantity inside the sum in the text above)\n",
    "    ### FILL IN CODE HERE!  Implement the log-likelihood function from the text above!\n",
    "    loglikes = #log.( .... ) -. 0.5 .* ().^ ./ ().^2\n",
    "    # the log-likelihood for the whole vector of measurements is the sum of individual log-likelihoods\n",
    "    loglike = sum(loglikes)\n",
    "    return loglike\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Maximum likelihood</h2>\n",
    "\n",
    "Before we start experimenting Markov Chain Monte Carlo, let's use a\n",
    "stock optimizer routine from Julia's *Optim* package.  The optimizer will allow us to find the *maximum likelihood* paramaters $b$ and $m$.\n",
    "\n",
    "Since the optimizer wants to *minimize*\n",
    "a function but we want to *maximize* the log-likelihood, we need to add a\n",
    "negative sign..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The optimizer we're using here requires an initial guess.  This log-likelihood happens\n",
    "# to be pretty simple, so we don't need to work very hard to give it a good initial guess!\n",
    "initial_params = [0., 0.]\n",
    "# The \"args\" parameter here gets passed to the neg_ll_one function (after the parameters)\n",
    "result = optimize(p -> -log_likelihood_one(p, x, y, yerr), initial_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_ml,m_ml = Optim.minimizer(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we used the jack-knife routine to estimate variances on our parameters.\n",
    "We can also compute the second derivative of the log-likelihood function, at the peak,\n",
    "to get an estimate of uncertainties on the parameters.  This is related to the Fisher\n",
    "information matrix, if you've ever encountered that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't worry about understanding this!\n",
    "invhess = inv(ForwardDiff.hessian(p -> -log_likelihood_one(p, x, y, yerr), [b_ml,m_ml]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The optimizer gives us the parameters that maximize the log-likelihood, along with an estimate of the uncertainties.\n",
    "# To start, let's have a look at our data set.\n",
    "f = Figure()\n",
    "Axis(f[1, 1], xlabel=\"B\", ylabel=\"M\")\n",
    "errorbars!(x, y, yerr);\n",
    "scatter!(x, y, markersize=20);\n",
    "xx = LinRange(50, 250, 50)\n",
    "lines!(xx, b_ml .+ m_ml .* xx)\n",
    "# Draw a sampling of B,M parameter values that are consistent with the fit,\n",
    "# using the estimated inverse-Hessian matrix (parameter covariance)\n",
    "using LinearAlgebra\n",
    "# use the svd to draw multivariate random normal samples!\n",
    "S = svd(invhess)\n",
    "BM = [S.U * Diagonal(sqrt.(S.S)) * randn(2) for i in 1:10]\n",
    "for (db,dm) in BM\n",
    "    lines!(xx, (b_ml .+ db) .+ (m_ml .+ dm) .* xx, color=:cornflowerblue)\n",
    "end\n",
    "lines!(xx, b_ml .+ m_ml .* xx, color=:black, linewidth=5)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also plot the ellipse showing the constraints in B,M space by manipulating hess_inv\n",
    "# (don't worry about understanding this math)\n",
    "SS = S.U * Diagonal(sqrt.(S.S))\n",
    "th = LinRange(0., 2Ï€, 200)\n",
    "xx = sin.(th)\n",
    "yy = cos.(th)\n",
    "dbm = SS * [xx yy]'\n",
    "ellipse_b = b_ml .+ dbm[1,:]\n",
    "ellipse_m = m_ml .+ dbm[2,:]\n",
    "\n",
    "f = Figure()\n",
    "Axis(f[1, 1], xlabel=\"B\", ylabel=\"M\", title=\"Parameter constraints from Maximum-Likelihood\")\n",
    "lines!(ellipse_b, ellipse_m, color=:red)\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Markov Chain Monte Carlo</h2>\n",
    "\n",
    "Next, let's implement the Markov Chain Monte Carlo algorithm.\n",
    "\n",
    "The MCMC algorithm moves a \"particle\" or \"sample\" or \"walker\" randomly around the particle space, by first proposing a move, and then using the relative likelihoods of the current and proposed positions to decide whether to accept or reject the move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function mcmc(logprob_func,\n",
    "              propose_func,\n",
    "              initial_pos, nsteps)\n",
    "    \"\"\"\n",
    "    MCMC: Markov Chain Monte Carlo.  Draw samples from the *logprob_func* probability distribution,\n",
    "    using proposed moves generated by the function *propose_func*.\n",
    "\n",
    "    * logprob_func: a function that returns the log-probability at a given value of parameters.\n",
    "               It will get called like this:\n",
    "        lnp = logprob_func(params, logprob_args)\n",
    "    * propose_func: a function that proposes to jump to a new point in parameter space.\n",
    "               It will get called like this:\n",
    "        p_new = propose_func(p, propose_args)\n",
    "    * initial_pos: initial position in parameter space (list/array)\n",
    "    * nsteps: integer number of MCMC steps to take\n",
    "    \n",
    "    Returns  (chain, faccept)\n",
    "    * chain: size Nsteps x P, MCMC samples\n",
    "    * faccept: float: fraction of proposed jumps that were accepted\n",
    "    \"\"\"\n",
    "    p = initial_pos\n",
    "    logprob = logprob_func(p)\n",
    "    chain = zeros(Float64, (nsteps, length(p)))\n",
    "    naccept = 0\n",
    "    for i in 1:nsteps\n",
    "        # propose a new position in parameter space\n",
    "        ### FILL IN CODE HERE -- propose a jump to a new place in param space\n",
    "        p_new = #...\n",
    "        # compute probability at new position\n",
    "        ### FILL IN CODE HERE\n",
    "        logprob_new = #...\n",
    "        # decide whether to jump to the new position\n",
    "        #### FILL IN CODE HERE!!!\n",
    "        if exp( ..... ) > rand()\n",
    "            p = p_new\n",
    "            logprob = logprob_new\n",
    "            naccept += 1\n",
    "        end\n",
    "        # save the position\n",
    "        chain[i,:] = p\n",
    "    end\n",
    "    return chain, naccept/nsteps\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a Gaussian (without covariance between the parameters) for our proposal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function propose_gaussian(p, stdevs)\n",
    "    \"\"\"\n",
    "    A Gaussian proposal distribution for mcmc.\n",
    "    *p*: the point in parameter space to jump from\n",
    "    *stdevs*: standard deviations for each dimension in the parameter space.\n",
    "    \"\"\"\n",
    "    return p .+ randn(length(p)) .* stdevs\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we defined our log-likelihood function above, but when using MCMC for Bayesian inference, we need to pass it a log-*posterior* function.  That is,\n",
    "we must include the log-prior for the parameters.  It is very common to see \"uninformative\" or \"flat\" priors used; in fact, it's not uncommon to see the log-prior just set to zero, as below, which is, statistically speaking, a naughty thing to do, since that prior definitely isn't a proper probability distribution -- it isn't even bounded!  But, it *feels* like we haven't imposed our *subjective* prior beliefs on the inference, which is why people often do it.  But you can't avoid subjectivity---if you change the parameterization, for example, a flat prior becomes non-flat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function log_posterior_one(params, x, y, err)\n",
    "    loglike = log_likelihood_one(params, x, y, yerr)\n",
    "    # Improper, flat priors on params!\n",
    "    logprior = 0.\n",
    "    return loglike + logprior\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial B,M\n",
    "initial_pos = [0., 1.0]\n",
    "# proposal distribution: jump sizes for B,M\n",
    "jump_sizes = [1., 0.1]\n",
    "# Run MCMC!\n",
    "chain,accept = mcmc(p -> log_posterior_one(p, x, y, err),\n",
    "                    p -> propose_gaussian(p, jump_sizes),\n",
    "                    initial_pos, 5000)\n",
    "println(\"Fraction of moves accepted:\", accept)\n",
    "size(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the parameter values in the chain!\n",
    "f = Figure()\n",
    "Axis(f[1, 1], xlabel=\"B\", ylabel=\"M\", title=\"MCMC Samples\")\n",
    "scatter!(chain[:,1], chain[:,2], color=:grey)\n",
    "lines!(ellipse_b, ellipse_m, color=:red)\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try re-running the MCMC cell above and re-plotting the results.  Do the results look the same every time?  Does that suggest anything to you about whether the chain has *converged* after the number of steps we have taken?\n",
    "\n",
    "Try increasing the number of steps -- do the results look better?  How long are you willing to wait?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the MCMC chains with respect to sample number\n",
    "f = Figure(size=(1600, 500))\n",
    "Axis(f[1, 1], ylabel=\"B\", xlabel=\"MCMC Step\")\n",
    "lines!(chain[:,1])\n",
    "Axis(f[1, 2], ylabel=\"M\", xlabel=\"MCMC Step\")\n",
    "lines!(chain[:,2])\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these plots of how the \"particle\" moves through the $B$,$M$ parameter space, what do you see?  How often does it traverse the whole space?  Do you think the step sizes are too big or too small?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom in on the beginning of the chain to see the \"burn-in\", and repeated values\n",
    "# Plot the MCMC chains with respect to sample number\n",
    "f = Figure(size=(1600, 500))\n",
    "Axis(f[1, 1], ylabel=\"B\", xlabel=\"MCMC Step\")\n",
    "nstart=200\n",
    "lines!(chain[1:nstart,1])\n",
    "Axis(f[1, 2], ylabel=\"M\", xlabel=\"MCMC Step\")\n",
    "lines!(chain[1:nstart,2])\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plots above, observe that our MCMC algorithm is moving toward the \"core\" of the probability mass.  Also observe that there are horizontal segments in the plots --- where we have proposed a number of moves that have been rejected, so that there are repeated `B,M` values in our MCMC chain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size(chain,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the models for a few randomly drawn MCMC samples.\n",
    "# The optimizer gives us the parameters that maximize the log-likelihood, along with an estimate of the uncertainties.\n",
    "# To start, let's have a look at our data set.\n",
    "f = Figure()\n",
    "Axis(f[1, 1], xlabel=\"X\", ylabel=\"Y\", title=\"Sampling of MCMC fits\")\n",
    "errorbars!(x, y, yerr);\n",
    "scatter!(x, y, markersize=20, color=:black);\n",
    "xx = LinRange(50, 250, 50)\n",
    "\n",
    "# Drop this many samples for \"burn-in\"\n",
    "nburn = 1000\n",
    "for i in rand(nburn:size(chain,1), 10)\n",
    "    # Draw some random entries from the chain\n",
    "    b,m = chain[i,:]\n",
    "    lines!(xx, b .+ m .* xx, color=:cornflowerblue)\n",
    "end\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing we can do is histogram the $B$ and $M$ values.  In fact, let's make a \"corner plot\" where we also show the *marginal* distributions.  We'll use my little homebrewed `cornerplot` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function cornerplot(x, names; resolution=(600,600))\n",
    "    # how many columns of data\n",
    "    dim = size(x, 2)\n",
    "    # rows to plot\n",
    "    idxs = 1:size(x,1)\n",
    "    f = Figure(resolution=resolution)\n",
    "    for i in 1:dim, j in 1:dim\n",
    "        if i < j\n",
    "            continue\n",
    "        end\n",
    "        ax = Axis(f[i, j], aspect = 1,\n",
    "                  topspinevisible = false,\n",
    "                  rightspinevisible = false,)\n",
    "        if i == j\n",
    "            hist!(x[idxs,i], direction=:y)\n",
    "            ax.xlabel = names[i]\n",
    "        else\n",
    "            scatter!(x[idxs,j], x[idxs,i], markersize=4)\n",
    "            ax.xlabel = names[j]\n",
    "            ax.ylabel = names[i]\n",
    "        end\n",
    "    end\n",
    "    f\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cornerplot(chain, [\"B\",\"M\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try calling the MCMC routine again with step sizes of [0.1, 0.01]\n",
    "and see what the plots look like!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tuning MCMC proposal distribution step sizes</h3>\n",
    "\n",
    "If we have two parameters, how do we know which step size we should adjust in\n",
    "order to get a good acceptance ratio?\n",
    "\n",
    "One approach is to modify our MCMC function so that instead of stepping in both\n",
    "parameters at once, we alternate and step in only one parameter at each step of the algorithm.  We could try to write a fancy general version of that, but instead let's just copy-paste the MCMC routine and customize it for this task!  Once we've selected good step sizes we can go back to the regular version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function mcmc_cyclic(logprob_func,\n",
    "                     propose_func,\n",
    "                     initial_pos, nsteps)\n",
    "    \"\"\"\n",
    "    This is a variation on the \"vanilla\" MCMC algorithm, where we change the proposal function\n",
    "    to modify only a single parameter in each step of the MCMC.  We record the acceptance ratio\n",
    "    separately for each parameter.\n",
    "    \"\"\"\n",
    "    p = initial_pos\n",
    "    logprob = logprob_func(p)\n",
    "    chain = zeros(Float64, (nsteps, length(p)))\n",
    "    naccept = zeros(Int, length(p))\n",
    "    for i = 1:nsteps\n",
    "        # propose a new position in parameter space\n",
    "        p_jump = propose_func(p)\n",
    "        # BUT, only copy one element (cycle through the elements);\n",
    "        # keep the rest the same!\n",
    "        # The index of the parameter to change:\n",
    "        j = 1 + ((i-1) % length(p))\n",
    "        p_new = copy(p)\n",
    "        p_new[j] = p_jump[j]\n",
    "        # compute probability at new position\n",
    "        logprob_new = logprob_func(p_new)\n",
    "        # decide whether to jump to the new position\n",
    "        if exp(logprob_new - logprob) > rand()\n",
    "            p = p_new\n",
    "            logprob = logprob_new\n",
    "            naccept[j] += 1\n",
    "        end\n",
    "        # save the position\n",
    "        chain[i,:] = p\n",
    "    end\n",
    "    # Since we cycle through the parameters, the number of steps per parameter\n",
    "    # is (approximately) (nsteps) / (number of parameters).\n",
    "    return chain, naccept/(nsteps/length(p))\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial B,M\n",
    "initial_pos = [0., 1.0]\n",
    "# proposal distribution: jump sizes for B,M\n",
    "##### CHANGE THESE -- we were using [1, 0.1] before.  Play around with these values until you get\n",
    "##### acceptance ratios of about 0.5 per coordinate!\n",
    "jump_sizes = [1., 0.1]\n",
    "# Run MCMC!\n",
    "chain,accept = mcmc_cyclic(\n",
    "    p -> log_posterior_one(p, x, y, yerr),\n",
    "    p -> propose_gaussian(p, jump_sizes),\n",
    "    initial_pos, 5000)\n",
    "println(\"Fraction of moves accepted (for B & M, respective):\", accept)\n",
    "size(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try modifying the `jump_sizes` values above until you get acceptance ratios of about 0.5 for each parameter.  Recall that proposing smaller jumps should result in a larger acceptance ratio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's plug those *jump_sizes* into our \"vanilla\" MCMC.  Since we are now jumping in both parameters\n",
    "# at once, the acceptance ratio will be a bit smaller.\n",
    "# initial B,M\n",
    "initial_pos = [0., 1.0]\n",
    "# proposal distribution: jump sizes for B,M\n",
    "jump_sizes #=  FROM ABOVE\n",
    "# Run MCMC!\n",
    "chain,accept = mcmc(log_posterior_one, (x,y,yerr),\n",
    "                    propose_gaussian, jump_sizes,\n",
    "                    initial_pos, 5000)\n",
    "println(\"Fraction of moves accepted:\", accept)\n",
    "size(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the resulting samples!\n",
    "f = Figure(size=(1600, 500))\n",
    "Axis(f[1, 1], ylabel=\"B\", xlabel=\"MCMC Step\")\n",
    "lines!(chain[:,1])\n",
    "Axis(f[1, 2], ylabel=\"M\", xlabel=\"MCMC Step\")\n",
    "lines!(chain[:,2])\n",
    "Axis(f[1, 3], xlabel=\"B\", ylabel=\"M\")\n",
    "scatter!(chain[:,1], chain[:,2])\n",
    "# Also plot up the elliptical constraint we got from the optimizer.\n",
    "lines!(ellipse_b, ellipse_m, color=:red, linewidth=5)\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks way better!  Our samples are traversing the state space many times.\n",
    "\n",
    "Let's also look at the resulting corner plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nburn = 1000\n",
    "cornerplot(chain, [\"B\",\"M\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Extensions</h3>\n",
    "\n",
    "* <b>If you are interested, try extending the model from a linear model to a quadratic model.  That is, switch to $y_{\\textrm{pred}} = b + m x + q x^2$.  You will need to write a new `log_likelihood_quadratic` function that expects three parameters.  Run MCMC on that model, plot the results, and show the corner plots.  Does it look like the model \"needs\" the quadratic term?</b>\n",
    "\n",
    "* <b>What if the $\\sigma$ values are estimated incorrectly?  (Eg, if your experimenter friends overlooked or mis-estimated some source of error in their data collection!)  Try increasing or decreasing the `yerr` values by a factor of 2 and re-make the plots.  How do the constraints on $B$ and $M$ change?  How does the visual quality of the fit change?\n",
    "\n",
    "* <b>We found jump sizes for $B$ and $M$ the led to okay acceptance ratios, but we are still taking jumps independently in the two variables, while we can clearly see that the variables are correlated.  Can you come up with a new `propose_func` that proposes jumps drawn from a Gaussian with appropriate covariance?  (you can check out where I sample from the inverse-Hessian ellipse, above, for how to sample from a multivariate Gaussian distribution given its covariance).\n",
    "\n",
    "* <b>We used \"uninformative\" priors on $B$ and $M$.  Try changing that -- for example, try placing a Gaussian (log-)prior on one of the parameters, and see how that affects your samplings.  How strong do you have to make the prior for it to have a significant effect on your results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope this has been an interesting glimpse into Markov Chain Monte Carlo in practice!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
